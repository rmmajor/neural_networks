{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network implementation.\n",
    "\n",
    "This homework shamelessly demands you to implement DQN — an approximate Q-learning algorithm with experience replay and target networks — and see if it works any better this way.\n",
    "\n",
    "Original paper:\n",
    "https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you find it easy to adapt it to almost any Python-based deep learning framework."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import random\n",
    "from reinforcement_learning import utils"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:12:37.723967Z",
     "start_time": "2024-05-26T12:12:37.718010Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole again\n",
    "\n",
    "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
    "\n",
    "CartPole is the simplest one. It should take several minutes to solve it.\n",
    "\n",
    "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T11:29:53.591815Z",
     "start_time": "2024-05-26T11:29:53.588856Z"
    }
   },
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\").unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T11:29:55.809133Z",
     "start_time": "2024-05-26T11:29:54.109994Z"
    }
   },
   "source": [
    "env = make_env()\n",
    "env.reset()\n",
    "plt.imshow(env.render())\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF7CAYAAAD4/3BBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAApXUlEQVR4nO3dfXTU5Z338c9MkhkewkwaIJmkJIiKQIRgFzDM2rq0pIQHrazxHLUsYMuRWzbxFKMW06Uido9xdc/60EX4Y7vi3kdKpUd0pYLFIGHVgBrJ8qCmws02WJiEQjND0DzNXPcfHubsKA+ZEGauJO/XOb9zMr/rOzPf33Ug+Zzfo8MYYwQAAGARZ7IbAAAA+CoCCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwTlIDypo1a3TFFVdo0KBBKioq0nvvvZfMdgAAgCWSFlB+85vfqKKiQqtWrdKHH36oyZMnq6SkRM3NzclqCQAAWMKRrIcFFhUVadq0afrXf/1XSVIkElFeXp7uvfdePfTQQ8loCQAAWCI1GV/a0dGhuro6VVZWRtc5nU4VFxertrb2a/Xt7e1qb2+Pvo5EIjp16pSGDx8uh8ORkJ4BAMClMcbo9OnTys3NldN54YM4SQkof/7znxUOh5WdnR2zPjs7W5988snX6quqqrR69epEtQcAAC6jo0ePatSoUResSUpAiVdlZaUqKiqir4PBoPLz83X06FF5PJ4kdgYAALorFAopLy9Pw4YNu2htUgLKiBEjlJKSoqamppj1TU1N8vl8X6t3u91yu91fW+/xeAgoAAD0Md05PSMpV/G4XC5NmTJF1dXV0XWRSETV1dXy+/3JaAkAAFgkaYd4KioqtHjxYk2dOlXXX3+9nn76aZ05c0Y/+tGPktUSAACwRNICyu23364TJ07o4YcfViAQ0HXXXadt27Z97cRZAAAw8CTtPiiXIhQKyev1KhgMcg4KAAB9RDx/v3kWDwAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdXo9oDzyyCNyOBwxy/jx46PjbW1tKisr0/Dhw5Wenq7S0lI1NTX1dhsAAKAPuyx7UK699lodP348urz99tvRsfvuu0+vvfaaNm3apJqaGh07dky33nrr5WgDAAD0UamX5UNTU+Xz+b62PhgM6le/+pU2bNig733ve5Kk559/XhMmTNDu3bs1ffr0y9EOAADoYy7LHpRPP/1Uubm5uvLKK7VgwQI1NjZKkurq6tTZ2ani4uJo7fjx45Wfn6/a2trzfl57e7tCoVDMAgAA+q9eDyhFRUVav369tm3bprVr1+rIkSP6zne+o9OnTysQCMjlcikjIyPmPdnZ2QoEAuf9zKqqKnm93uiSl5fX220DAACL9Pohnjlz5kR/LiwsVFFRkUaPHq2XXnpJgwcP7tFnVlZWqqKiIvo6FAoRUgAA6Mcu+2XGGRkZuuaaa3To0CH5fD51dHSopaUlpqapqemc56yc5Xa75fF4YhYAANB/XfaA0traqsOHDysnJ0dTpkxRWlqaqquro+MNDQ1qbGyU3++/3K0AAIA+otcP8TzwwAO6+eabNXr0aB07dkyrVq1SSkqK7rzzTnm9Xi1ZskQVFRXKzMyUx+PRvffeK7/fzxU8AAAgqtcDymeffaY777xTJ0+e1MiRI/Xtb39bu3fv1siRIyVJTz31lJxOp0pLS9Xe3q6SkhI999xzvd0GAADowxzGGJPsJuIVCoXk9XoVDAY5HwUAgD4inr/fPIsHAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCduAPKrl27dPPNNys3N1cOh0OvvPJKzLgxRg8//LBycnI0ePBgFRcX69NPP42pOXXqlBYsWCCPx6OMjAwtWbJEra2tl7QhAACg/4g7oJw5c0aTJ0/WmjVrzjn+xBNP6Nlnn9W6deu0Z88eDR06VCUlJWpra4vWLFiwQAcPHtT27du1ZcsW7dq1S0uXLu35VgAAgH7FYYwxPX6zw6HNmzdr/vz5kr7ce5Kbm6v7779fDzzwgCQpGAwqOztb69ev1x133KGPP/5YBQUFev/99zV16lRJ0rZt2zR37lx99tlnys3Nvej3hkIheb1eBYNBeTyenrYPAAASKJ6/3716DsqRI0cUCARUXFwcXef1elVUVKTa2lpJUm1trTIyMqLhRJKKi4vldDq1Z8+ec35ue3u7QqFQzAIAAPqvXg0ogUBAkpSdnR2zPjs7OzoWCASUlZUVM56amqrMzMxozVdVVVXJ6/VGl7y8vN5sGwAAWKZPXMVTWVmpYDAYXY4ePZrslgAAwGXUqwHF5/NJkpqammLWNzU1Rcd8Pp+am5tjxru6unTq1KlozVe53W55PJ6YBQAA9F+9GlDGjBkjn8+n6urq6LpQKKQ9e/bI7/dLkvx+v1paWlRXVxet2bFjhyKRiIqKinqzHQAA0EelxvuG1tZWHTp0KPr6yJEjqq+vV2ZmpvLz87V8+XL94z/+o8aOHasxY8bo5z//uXJzc6NX+kyYMEGzZ8/W3XffrXXr1qmzs1Pl5eW64447unUFDwAA6P/iDigffPCBvvvd70ZfV1RUSJIWL16s9evX66c//anOnDmjpUuXqqWlRd/+9re1bds2DRo0KPqeF198UeXl5Zo5c6acTqdKS0v17LPP9sLmAACA/uCS7oOSLNwHBQCAvidp90EBAADoDQQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWiTug7Nq1SzfffLNyc3PlcDj0yiuvxIzfddddcjgcMcvs2bNjak6dOqUFCxbI4/EoIyNDS5YsUWtr6yVtCAAA6D/iDihnzpzR5MmTtWbNmvPWzJ49W8ePH48uv/71r2PGFyxYoIMHD2r79u3asmWLdu3apaVLl8bfPQAA6JdS433DnDlzNGfOnAvWuN1u+Xy+c459/PHH2rZtm95//31NnTpVkvTLX/5Sc+fO1T//8z8rNzc33pYAAEA/c1nOQdm5c6eysrI0btw4LVu2TCdPnoyO1dbWKiMjIxpOJKm4uFhOp1N79uw55+e1t7crFArFLAAAoP/q9YAye/Zs/cd//Ieqq6v1T//0T6qpqdGcOXMUDoclSYFAQFlZWTHvSU1NVWZmpgKBwDk/s6qqSl6vN7rk5eX1dtsAAMAicR/iuZg77rgj+vOkSZNUWFioq666Sjt37tTMmTN79JmVlZWqqKiIvg6FQoQUAAD6sct+mfGVV16pESNG6NChQ5Ikn8+n5ubmmJquri6dOnXqvOetuN1ueTyemAUAAPRflz2gfPbZZzp58qRycnIkSX6/Xy0tLaqrq4vW7NixQ5FIREVFRZe7HQAA0AfEfYintbU1ujdEko4cOaL6+nplZmYqMzNTq1evVmlpqXw+nw4fPqyf/vSnuvrqq1VSUiJJmjBhgmbPnq27775b69atU2dnp8rLy3XHHXdwBQ8AAJAkOYwxJp437Ny5U9/97ne/tn7x4sVau3at5s+fr71796qlpUW5ubmaNWuWfvGLXyg7Oztae+rUKZWXl+u1116T0+lUaWmpnn32WaWnp3erh1AoJK/Xq2AwyOEeAAD6iHj+fscdUGxAQAEAoO+J5+83z+IBAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOvE/bBAAOgtxhgd+v1amUj4gnVXfu/HSnUPTVBXAGxAQAGQNJGuTgWPHpAJd1247iLjAPofDvEASJpIZ5vU5x5XCiARCCgAkibc2Z7sFgBYioACIGkinW1iFwqAcyGgAEga9qAAOB8CCoCkCXe2JbsFAJYioABImkhXu4zhEA+AryOgAEgaDvEAOB8CCoCkCR09KF1kD8rQ7KvkTOGWTcBAQ0ABkDRfnPqTLnYVz5Dho+RwElCAgYaAAsBqKWluORyOZLcBIMEIKACs5kx1SSKgAAMNAQWA1Zypbok9KMCAQ0ABYDVnqouAAgxABBQAVktJc3OABxiACCgArMYeFGBgIqAASIru3kHWmeoWJ8kCAw8BBUBSmEhEphtPMnY4U7jMGBiACCgAkiIS7rjoXWQBDFxxBZSqqipNmzZNw4YNU1ZWlubPn6+GhoaYmra2NpWVlWn48OFKT09XaWmpmpqaYmoaGxs1b948DRkyRFlZWXrwwQfV1dV16VsDoM8wXR0yJpLsNgBYKq6AUlNTo7KyMu3evVvbt29XZ2enZs2apTNnzkRr7rvvPr322mvatGmTampqdOzYMd16663R8XA4rHnz5qmjo0PvvvuuXnjhBa1fv14PP/xw720VAOuFu9iDAuD8HOYSnnV+4sQJZWVlqaamRjfeeKOCwaBGjhypDRs26LbbbpMkffLJJ5owYYJqa2s1ffp0bd26VTfddJOOHTum7OxsSdK6deu0YsUKnThxQi6X66LfGwqF5PV6FQwG5fF4eto+gCT64tQx/WHbL9Vx+uQF666Zu1zevIIEdQXgcorn7/clnYMSDAYlSZmZmZKkuro6dXZ2qri4OFozfvx45efnq7a2VpJUW1urSZMmRcOJJJWUlCgUCungwYPn/J729naFQqGYBUDfxh4UABfS44ASiUS0fPly3XDDDZo4caIkKRAIyOVyKSMjI6Y2OztbgUAgWvO/w8nZ8bNj51JVVSWv1xtd8vLyeto2AEtEwpyDAuD8ehxQysrKdODAAW3cuLE3+zmnyspKBYPB6HL06NHL/p0ALi/T1ckeFADnldqTN5WXl2vLli3atWuXRo0aFV3v8/nU0dGhlpaWmL0oTU1N8vl80Zr33nsv5vPOXuVztuar3G633G53T1oFYKnOL04rEr7w1XvOVJccKSkJ6giATeLag2KMUXl5uTZv3qwdO3ZozJgxMeNTpkxRWlqaqquro+saGhrU2Ngov98vSfL7/dq/f7+am5ujNdu3b5fH41FBASfCAQNF6LODCrefuWDNsJxxcg8bkaCOANgkrj0oZWVl2rBhg1599VUNGzYses6I1+vV4MGD5fV6tWTJElVUVCgzM1Mej0f33nuv/H6/pk+fLkmaNWuWCgoKtHDhQj3xxBMKBAJauXKlysrK2EsCIIYjNVUOJ/eTBAaiuALK2rVrJUkzZsyIWf/888/rrrvukiQ99dRTcjqdKi0tVXt7u0pKSvTcc89Fa1NSUrRlyxYtW7ZMfr9fQ4cO1eLFi/Xoo49e2pYA6HecKWlyODjEAwxEl3QflGThPihA3/f/dvxKJz/dc8GaEeNu0KjppUoblJ6grgBcTgm7DwoAXE4O9qAAAxYBBYC1nClpnIMCDFD8zwdgLScnyQIDFv/zASRcd0994xAPMHARUAAkgelWSHE4UySHIwH9ALANAQVAwplwl0wk3K1aBwEFGJAIKAASLhIJdzugABiYCCgAEu7LPSgXfg4PgIGNgAIg4Uyk+4d4AAxMBBQACRcJd8mECSgAzo+AAiDh4jlJFsDAREABkHCGk2QBXAQBBUDCRcJdihBQAFwAAQVAwn3+5z+q7S/HLlgzyJut9OwrE9QRANsQUAAkXKQb56A409xKcQ1JUEcAbENAAWAlh8MpZ0pqstsAkCQEFABWcjhT5CCgAAMWAQWAlRzOFDmdBBRgoCKgALASe1CAgY2AAsBKXwaUtGS3ASBJCCgArORwpnCSLDCAEVAAJJQxRjKRi9Y5nE45nCkJ6AiAjQgoABLLRBQJd3Wj0CGHw3HZ2wFgJwIKgIQyJiIT7kx2GwAsR0ABkFAmElGEgALgIggoABLLRBTpIqAAuDACCoCEMhEO8QC4OAIKgIQy7EEB0A0EFAAJZQznoAC4OAIKgISKdHao8/PghYscTqUOSk9MQwCsREABkFCdn7eoNXDogjUprsHy5l2boI4A2CiugFJVVaVp06Zp2LBhysrK0vz589XQ0BBTM2PGDDkcjpjlnnvuialpbGzUvHnzNGTIEGVlZenBBx9UV1d3btwEYCBwOBxyprqS3QaAJIrrQRc1NTUqKyvTtGnT1NXVpZ/97GeaNWuWPvroIw0dOjRad/fdd+vRRx+Nvh4yZEj053A4rHnz5snn8+ndd9/V8ePHtWjRIqWlpemxxx7rhU0C0Oc5HHIQUIABLa6Asm3btpjX69evV1ZWlurq6nTjjTdG1w8ZMkQ+n++cn/H73/9eH330kd58801lZ2fruuuu0y9+8QutWLFCjzzyiFwufikBA51DDqUQUIAB7ZLOQQkGvzzRLTMzM2b9iy++qBEjRmjixImqrKzU559/Hh2rra3VpEmTlJ2dHV1XUlKiUCikgwcPnvN72tvbFQqFYhYA/ZjDKWdqWrK7AJBEPX6WeSQS0fLly3XDDTdo4sSJ0fU//OEPNXr0aOXm5mrfvn1asWKFGhoa9PLLL0uSAoFATDiRFH0dCATO+V1VVVVavXp1T1sF0NdwDgow4PU4oJSVlenAgQN6++23Y9YvXbo0+vOkSZOUk5OjmTNn6vDhw7rqqqt69F2VlZWqqKiIvg6FQsrLy+tZ4wCsx0myAHp0iKe8vFxbtmzRW2+9pVGjRl2wtqioSJJ06NCXlxX6fD41NTXF1Jx9fb7zVtxutzweT8wCoB9zOORMIaAAA1lcAcUYo/Lycm3evFk7duzQmDFjLvqe+vp6SVJOTo4kye/3a//+/Wpubo7WbN++XR6PRwUFBfG0A6CPMcZ0q87hcMqZRkABBrK4DvGUlZVpw4YNevXVVzVs2LDoOSNer1eDBw/W4cOHtWHDBs2dO1fDhw/Xvn37dN999+nGG29UYWGhJGnWrFkqKCjQwoUL9cQTTygQCGjlypUqKyuT2+3u/S0EYJXu3ube4eA+ksBAFtdvgLVr1yoYDGrGjBnKycmJLr/5zW8kSS6XS2+++aZmzZql8ePH6/7771dpaalee+216GekpKRoy5YtSklJkd/v19/93d9p0aJFMfdNAdBfGUU6O5LdBIA+IK49KBfbPZuXl6eampqLfs7o0aP1+uuvx/PVAPoDI4W72pPdBYA+gH2oABLIKNJJQAFwcQQUAAljJIUJKAC6gYACIHGMUYRDPAC6gYACIIE4xAOgewgoABLHiD0oALqFgAIgYSLhDp345J2LVDk0csKNF6kB0N8RUAAklOnGjdrSBg9LQCcAbEZAAWAXh+RM467SwEBHQAFgHWfaoGS3ACDJCCgArJPCHhRgwCOgALBOSioBBRjoCCgALOOQ08UhHmCgI6AAsE4K56AAAx4BBYB1CCgACCgAEsIYo0i4q1u1Die/moCBjt8CABIm3NGW7BYA9BEEFAAJw4MCAXQXAQVAwoQ72YMCoHsIKAAShoACoLtSk90AgL4jHA7LGNPj93e2fd6tuq6usBzOnn+P0+mUkxNtgT6NgAKg2xYtWqSXXnqpx+//wV9fo4d+eMMFa7q6ujRsWLrCkZ4HlAceeEBVVVU9fj+A5COgAOi2cDisrq7uXSp8Lu607u3V6OzqUuQSAko4HO7xewHYgYACIGGKJnwz+nOwa7j+0pmjjohbbucXGp72J6WnBrXvcEDqeTYB0E8QUAAkTOFVPklSU/to/eHzafoiPExhpSrF0amhzpAK0t/Rh3/48JLOcwHQP3AWGYCE+ktntva1zlBrOFNhpUlyKGxcCoVHqC5Uor+0DU12iwAsQEABkDCdxq3dwR+oy7jPMz5IE77zuOTgVxMw0PFbAECCOS5xHMBAQEABAADWIaAAAADrEFAAJEyao11TPa/LqXPfp8SpLv3m//4fGRNJcGcAbBNXQFm7dq0KCwvl8Xjk8Xjk9/u1devW6HhbW5vKyso0fPhwpaenq7S0VE1NTTGf0djYqHnz5mnIkCHKysrSgw8+eEk3fgLQt4xI+0yThu3UYOdpOdUlycipLg1xBjXVs1XtrY3JbhGABeK6D8qoUaP0+OOPa+zYsTLG6IUXXtAtt9yivXv36tprr9V9992n3/3ud9q0aZO8Xq/Ky8t166236p133pH05d0d582bJ5/Pp3fffVfHjx/XokWLlJaWpscee+yybCAAe/yu9g9yu1IlfaK/dH6gEx156jCDNch5RlmuP+ovqSfV0soDBQFIDnOJd0TKzMzUk08+qdtuu00jR47Uhg0bdNttt0mSPvnkE02YMEG1tbWaPn26tm7dqptuuknHjh1Tdna2JGndunVasWKFTpw4IZfL1a3vDIVC8nq9uuuuu7r9HgCXrrq6WocPH052Gxc1efJkFRUVJbsNAF/R0dGh9evXKxgMyuPxXLC2x3eSDYfD2rRpk86cOSO/36+6ujp1dnaquLg4WjN+/Hjl5+dHA0ptba0mTZoUDSeSVFJSomXLlungwYP61re+dc7vam9vV3t7e/R1KBSSJC1cuFDp6ek93QQAcTpy5EifCCiFhYVasmRJstsA8BWtra1av359t2rjDij79++X3+9XW1ub0tPTtXnzZhUUFKi+vl4ul0sZGRkx9dnZ2QoEApKkQCAQE07Ojp8dO5+qqiqtXr36a+unTp160QQGoPdkZmYmu4Vu8fl8uv7665PdBoCvOLuDoTvivopn3Lhxqq+v1549e7Rs2TItXrxYH330UbwfE5fKykoFg8HocvTo0cv6fQAAILni3oPicrl09dVXS5KmTJmi999/X88884xuv/12dXR0qKWlJWYvSlNTk3y+Lx8Q5vP59N5778V83tmrfM7WnIvb7Zbbfe5bYwMAgP7nku+DEolE1N7erilTpigtLU3V1dXRsYaGBjU2Nsrv90uS/H6/9u/fr+bm5mjN9u3b5fF4VFBQcKmtAACAfiKuPSiVlZWaM2eO8vPzdfr0aW3YsEE7d+7UG2+8Ia/XqyVLlqiiokKZmZnyeDy699575ff7NX36dEnSrFmzVFBQoIULF+qJJ55QIBDQypUrVVZWxh4SAAAQFVdAaW5u1qJFi3T8+HF5vV4VFhbqjTfe0Pe//31J0lNPPSWn06nS0lK1t7erpKREzz33XPT9KSkp2rJli5YtWya/36+hQ4dq8eLFevTRR3t3qwAAQJ8WV0D51a9+dcHxQYMGac2aNVqzZs15a0aPHq3XX389nq8FAAADDM/iAQAA1iGgAAAA6xBQAACAdQgoAADAOj1+Fg+AgWfatGkxz8Wy1cSJE5PdAoBLdMlPM06Gs08z7s7TEAEAgB3i+fvNIR4AAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6cQWUtWvXqrCwUB6PRx6PR36/X1u3bo2Oz5gxQw6HI2a55557Yj6jsbFR8+bN05AhQ5SVlaUHH3xQXV1dvbM1AACgX0iNp3jUqFF6/PHHNXbsWBlj9MILL+iWW27R3r17de2110qS7r77bj366KPR9wwZMiT6czgc1rx58+Tz+fTuu+/q+PHjWrRokdLS0vTYY4/10iYBAIC+zmGMMZfyAZmZmXryySe1ZMkSzZgxQ9ddd52efvrpc9Zu3bpVN910k44dO6bs7GxJ0rp167RixQqdOHFCLperW98ZCoXk9XoVDAbl8XgupX0AAJAg8fz97vE5KOFwWBs3btSZM2fk9/uj61988UWNGDFCEydOVGVlpT7//PPoWG1trSZNmhQNJ5JUUlKiUCikgwcPnve72tvbFQqFYhYAANB/xXWIR5L2798vv9+vtrY2paena/PmzSooKJAk/fCHP9To0aOVm5urffv2acWKFWpoaNDLL78sSQoEAjHhRFL0dSAQOO93VlVVafXq1fG2CgAA+qi4A8q4ceNUX1+vYDCo3/72t1q8eLFqampUUFCgpUuXRusmTZqknJwczZw5U4cPH9ZVV13V4yYrKytVUVERfR0KhZSXl9fjzwMAAHaL+xCPy+XS1VdfrSlTpqiqqkqTJ0/WM888c87aoqIiSdKhQ4ckST6fT01NTTE1Z1/7fL7zfqfb7Y5eOXR2AQAA/dcl3wclEomovb39nGP19fWSpJycHEmS3+/X/v371dzcHK3Zvn27PB5P9DARAABAXId4KisrNWfOHOXn5+v06dPasGGDdu7cqTfeeEOHDx/Whg0bNHfuXA0fPlz79u3TfffdpxtvvFGFhYWSpFmzZqmgoEALFy7UE088oUAgoJUrV6qsrExut/uybCAAAOh74goozc3NWrRokY4fPy6v16vCwkK98cYb+v73v6+jR4/qzTff1NNPP60zZ84oLy9PpaWlWrlyZfT9KSkp2rJli5YtWya/36+hQ4dq8eLFMfdNAQAAuOT7oCQD90EBAKDvSch9UAAAAC4XAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYJ3UZDfQE8YYSVIoFEpyJwAAoLvO/t0++3f8QvpkQDl9+rQkKS8vL8mdAACAeJ0+fVper/eCNQ7TnRhjmUgkooaGBhUUFOjo0aPyeDzJbqnPCoVCysvLYx57AXPZe5jL3sE89h7msncYY3T69Gnl5ubK6bzwWSZ9cg+K0+nUN7/5TUmSx+PhH0svYB57D3PZe5jL3sE89h7m8tJdbM/JWZwkCwAArENAAQAA1umzAcXtdmvVqlVyu93JbqVPYx57D3PZe5jL3sE89h7mMvH65EmyAACgf+uze1AAAED/RUABAADWIaAAAADrEFAAAIB1+mRAWbNmja644goNGjRIRUVFeu+995LdknV27dqlm2++Wbm5uXI4HHrllVdixo0xevjhh5WTk6PBgweruLhYn376aUzNqVOntGDBAnk8HmVkZGjJkiVqbW1N4FYkX1VVlaZNm6Zhw4YpKytL8+fPV0NDQ0xNW1ubysrKNHz4cKWnp6u0tFRNTU0xNY2NjZo3b56GDBmirKwsPfjgg+rq6krkpiTV2rVrVVhYGL3Jld/v19atW6PjzGHPPf7443I4HFq+fHl0HfPZPY888ogcDkfMMn78+Og485hkpo/ZuHGjcblc5t///d/NwYMHzd13320yMjJMU1NTsluzyuuvv27+4R/+wbz88stGktm8eXPM+OOPP268Xq955ZVXzH//93+bH/zgB2bMmDHmiy++iNbMnj3bTJ482ezevdv813/9l7n66qvNnXfemeAtSa6SkhLz/PPPmwMHDpj6+nozd+5ck5+fb1pbW6M199xzj8nLyzPV1dXmgw8+MNOnTzd//dd/HR3v6uoyEydONMXFxWbv3r3m9ddfNyNGjDCVlZXJ2KSk+M///E/zu9/9zvzhD38wDQ0N5mc/+5lJS0szBw4cMMYwhz313nvvmSuuuMIUFhaan/zkJ9H1zGf3rFq1ylx77bXm+PHj0eXEiRPRceYxufpcQLn++utNWVlZ9HU4HDa5ubmmqqoqiV3Z7asBJRKJGJ/PZ5588snoupaWFuN2u82vf/1rY4wxH330kZFk3n///WjN1q1bjcPhMH/6058S1rttmpubjSRTU1NjjPly3tLS0symTZuiNR9//LGRZGpra40xX4ZFp9NpAoFAtGbt2rXG4/GY9vb2xG6ARb7xjW+Yf/u3f2MOe+j06dNm7NixZvv27eZv/uZvogGF+ey+VatWmcmTJ59zjHlMvj51iKejo0N1dXUqLi6OrnM6nSouLlZtbW0SO+tbjhw5okAgEDOPXq9XRUVF0Xmsra1VRkaGpk6dGq0pLi6W0+nUnj17Et6zLYLBoCQpMzNTklRXV6fOzs6YuRw/frzy8/Nj5nLSpEnKzs6O1pSUlCgUCungwYMJ7N4O4XBYGzdu1JkzZ+T3+5nDHiorK9O8efNi5k3i32S8Pv30U+Xm5urKK6/UggUL1NjYKIl5tEGfeljgn//8Z4XD4Zh/DJKUnZ2tTz75JEld9T2BQECSzjmPZ8cCgYCysrJixlNTU5WZmRmtGWgikYiWL1+uG264QRMnTpT05Ty5XC5lZGTE1H51Ls8112fHBor9+/fL7/erra1N6enp2rx5swoKClRfX88cxmnjxo368MMP9f77739tjH+T3VdUVKT169dr3LhxOn78uFavXq3vfOc7OnDgAPNogT4VUIBkKisr04EDB/T2228nu5U+ady4caqvr1cwGNRvf/tbLV68WDU1Ncluq885evSofvKTn2j79u0aNGhQstvp0+bMmRP9ubCwUEVFRRo9erReeuklDR48OImdQepjV/GMGDFCKSkpXzuLuqmpST6fL0ld9T1n5+pC8+jz+dTc3Bwz3tXVpVOnTg3IuS4vL9eWLVv01ltvadSoUdH1Pp9PHR0damlpian/6lyea67Pjg0ULpdLV199taZMmaKqqipNnjxZzzzzDHMYp7q6OjU3N+uv/uqvlJqaqtTUVNXU1OjZZ59VamqqsrOzmc8eysjI0DXXXKNDhw7x79ICfSqguFwuTZkyRdXV1dF1kUhE1dXV8vv9SeysbxkzZox8Pl/MPIZCIe3Zsyc6j36/Xy0tLaqrq4vW7NixQ5FIREVFRQnvOVmMMSovL9fmzZu1Y8cOjRkzJmZ8ypQpSktLi5nLhoYGNTY2xszl/v37YwLf9u3b5fF4VFBQkJgNsVAkElF7eztzGKeZM2dq//79qq+vjy5Tp07VggULoj8znz3T2tqqw4cPKycnh3+XNkj2Wbrx2rhxo3G73Wb9+vXmo48+MkuXLjUZGRkxZ1HjyzP89+7da/bu3WskmX/5l38xe/fuNX/84x+NMV9eZpyRkWFeffVVs2/fPnPLLbec8zLjb33rW2bPnj3m7bffNmPHjh1wlxkvW7bMeL1es3PnzphLET///PNozT333GPy8/PNjh07zAcffGD8fr/x+/3R8bOXIs6aNcvU19ebbdu2mZEjRw6oSxEfeughU1NTY44cOWL27dtnHnroIeNwOMzvf/97YwxzeKn+91U8xjCf3XX//febnTt3miNHjph33nnHFBcXmxEjRpjm5mZjDPOYbH0uoBhjzC9/+UuTn59vXC6Xuf76683u3buT3ZJ13nrrLSPpa8vixYuNMV9eavzzn//cZGdnG7fbbWbOnGkaGhpiPuPkyZPmzjvvNOnp6cbj8Zgf/ehH5vTp00nYmuQ51xxKMs8//3y05osvvjB///d/b77xjW+YIUOGmL/92781x48fj/mc//mf/zFz5swxgwcPNiNGjDD333+/6ezsTPDWJM+Pf/xjM3r0aONyuczIkSPNzJkzo+HEGObwUn01oDCf3XP77bebnJwc43K5zDe/+U1z++23m0OHDkXHmcfkchhjTHL23QAAAJxbnzoHBQAADAwEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABY5/8DzFsKh46/Nv4AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to build a neural network that can map observations to state q-values.\n",
    "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:23:05.005976Z",
     "start_time": "2024-05-26T12:23:04.999559Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:23:06.100403Z",
     "start_time": "2024-05-26T12:23:06.094601Z"
    }
   },
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        # Define the network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "        # Initialize experience replay buffer attributes\n",
    "        self.buffer = []\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=1e-4)\n",
    "\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        qvalues = self.network(state_t)\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2 and \n",
    "            qvalues.shape[0] == state_t.shape[0] and \n",
    "            qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:23:06.915709Z",
     "start_time": "2024-05-26T12:23:06.909967Z"
    }
   },
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our agent to see if it raises any errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:23:08.119837Z",
     "start_time": "2024-05-26T12:23:08.117460Z"
    }
   },
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, trunk, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:23:08.903108Z",
     "start_time": "2024-05-26T12:23:08.872443Z"
    }
   },
   "source": [
    "evaluate(env, agent, n_games=1)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here in main notebook **to get 2 bonus points**.\n",
    "\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:23:19.173744Z",
     "start_time": "2024-05-26T12:23:19.163320Z"
    }
   },
   "source": [
    "from reinforcement_learning.replay_buffer import ReplayBuffer\n",
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    obs, _ = env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, _, _, _, _ = env.step(action)\n",
    "    exp_replay.add(obs, action, 1.0, next_obs, done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
    "\n",
    "obs_batch = torch.tensor(obs_batch, dtype=torch.float32).to(device)\n",
    "next_obs_batch = torch.tensor(next_obs_batch, dtype=torch.float32).to(device)\n",
    "reward_batch = torch.tensor(reward_batch, dtype=torch.float32).to(device)\n",
    "act_batch = torch.tensor(act_batch, dtype=torch.long).to(device)\n",
    "is_done_batch = torch.tensor(is_done_batch, dtype=torch.float32).to(device)\n",
    "\n",
    "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:23:20.135698Z",
     "start_time": "2024-05-26T12:23:20.131408Z"
    }
   },
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for _ in range(n_steps):\n",
    "        # Select an action using the agent\n",
    "        # Select an action using epsilon-greedy policy\n",
    "        if random.random() < agent.epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_tensor = torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            q_values = agent(state_tensor).detach().cpu().numpy()\n",
    "            action = np.argmax(q_values, axis=1)[0]\n",
    "        \n",
    "        # Perform the action in the environment\n",
    "        next_s, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # Record the experience in the replay buffer\n",
    "        exp_replay.add(s, action, reward, next_s, done)\n",
    "        \n",
    "        # Update the state and accumulated reward\n",
    "        s = next_s\n",
    "        sum_rewards += reward\n",
    "        \n",
    "        # If done, reset the environment and continue\n",
    "        if done:\n",
    "            s, _ = env.reset()\n",
    "\n",
    "    return sum_rewards, s"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T12:23:21.280145Z",
     "start_time": "2024-05-26T12:23:21.255298Z"
    }
   },
   "source": [
    "# testing your code.\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "state = env.reset()\n",
    "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1000, \\\n",
    "    \"play_and_record should have added exactly 1000 steps, \" \\\n",
    "    \"but instead added %i\" % len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "assert 0 < np.mean(is_dones) < 0.1, \\\n",
    "    \"Please make sure you restart the game whenever it is 'done' and \" \\\n",
    "    \"record the is_done correctly into the buffer. Got %f is_done rate over \" \\\n",
    "    \"%i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
    "        np.mean(is_dones), len(exp_replay))\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
    "    assert act_batch.shape == (10,), \\\n",
    "        \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (10,), \\\n",
    "        \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (10,), \\\n",
    "        \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
    "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
    "        \"is_done should be strictly True or False\"\n",
    "    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
    "\n",
    "print(\"Well done!\")"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m exp_replay \u001B[38;5;241m=\u001B[39m ReplayBuffer(\u001B[38;5;241m2000\u001B[39m)\n\u001B[1;32m      4\u001B[0m state \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m----> 5\u001B[0m \u001B[43mplay_and_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexp_replay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# if you're using your own experience replay buffer, some of those tests may need correction.\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# just make sure you know what your code does\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(exp_replay) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1000\u001B[39m, \\\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplay_and_record should have added exactly 1000 steps, \u001B[39m\u001B[38;5;124m\"\u001B[39m \\\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut instead added \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mlen\u001B[39m(exp_replay)\n",
      "Cell \u001B[0;32mIn[49], line 21\u001B[0m, in \u001B[0;36mplay_and_record\u001B[0;34m(initial_state, agent, env, exp_replay, n_steps)\u001B[0m\n\u001B[1;32m     19\u001B[0m     action \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39msample()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 21\u001B[0m     state_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     22\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m agent(state_tensor)\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     23\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(q_values, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mValueError\u001B[0m: expected sequence of length 4 at dim 1 (got 0)"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target networks\n",
    "\n",
    "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
    "\n",
    "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning\n",
    "Here we write a function similar to `agent.update` from tabular q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "\n",
    "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
    "\n",
    "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = <YOUR CODE>\n",
    "\n",
    "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
    "        \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    target_qvalues_for_actions = <YOUR CODE>\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
    "            \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim() == 1, \\\n",
    "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
    "            \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
    "\n",
    "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
    "                       agent, target_network,\n",
    "                       gamma=0.99, check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert loss.requires_grad and tuple(loss.data.size()) == (), \\\n",
    "    \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() != 0), \\\n",
    "    \"loss must be differentiable w.r.t. network weights\"\n",
    "assert np.all(next(target_network.parameters()).grad is None), \\\n",
    "    \"target network should not have grads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = <YOUR CODE: your favourite random seed>\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for something more complicated than CartPole\n",
    "\n",
    "# timesteps_per_epoch = 1\n",
    "# batch_size = 32\n",
    "# total_steps = 3 * 10**6\n",
    "# decay_steps = 1 * 10**6\n",
    "\n",
    "# opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "# init_epsilon = 1\n",
    "# final_epsilon = 0.1\n",
    "\n",
    "# loss_freq = 20\n",
    "# refresh_target_network_freq = 1000\n",
    "# eval_freq = 5000\n",
    "\n",
    "# max_grad_norm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print('less that 100 Mb RAM available, freezing')\n",
    "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        <YOUR CODE: sample batch_size of data >\n",
    "\n",
    "        loss = <YOUR CODE: compute TD loss>\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            <YOUR CODE>\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(evaluate(\n",
    "                make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues(\n",
    "                [make_env(seed=step).reset()]\n",
    "            )\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  make_env(),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)\n",
    "assert final_score > 300, 'not good enough for DQN'\n",
    "print('Well done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent's predicted V-values vs their Monte-Carlo estimates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = make_env()\n",
    "record = utils.play_and_log_episode(eval_env, agent)\n",
    "print('total reward for life:', np.sum(record['rewards']))\n",
    "for key in record:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(record['v_mc'], record['v_agent'])\n",
    "ax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n",
    "       'black', linestyle='--', label='x=y')\n",
    "\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title('State Value Estimates')\n",
    "ax.set_xlabel('Monte-Carlo')\n",
    "ax.set_ylabel('Agent')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
